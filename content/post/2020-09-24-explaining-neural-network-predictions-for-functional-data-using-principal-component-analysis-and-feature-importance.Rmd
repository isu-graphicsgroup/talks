---
title: Explaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Feature Importance
author: Katherine Goode
date: '2020-09-24'
slug: explaining-neural-network-predictions-for-functional-data-using-principal-component-analysis-and-feature-importance
categories: ["machine learning"]
tags: ["explainability", "XAI", "neural network"]
description: ''
images:
  - /post/2020-09-24-katherine.png
---

![](/post/2020-09-24-katherine.png)

Explainable machine learning has become a quickly growing area of research as the use of black-box models continues to increase. While many methods have been proposed, little research has been done relating to applications involving functional data. As an intern at Sandia National Laboratories, I have been helping to develop methods to provide explanations for an application focused on predicting explosive device characteristics using optical spectral-temporal signatures from explosions. In this talk, I’ll discuss our approach that involves transforming the functions using functional principal component analysis, training neural networks on the functional principal components, and using permutation feature importance (PFI) to identify the principal components that are important for prediction. Visualization has played a key role in the interpretation of the functional principal components identified as important by PFI to understand the functional variability in the signatures that is driving the predictions made by the neural networks.

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. SAND no: SAND2020-10057 A

The slides for the talk can be found [here](https://goodekat.github.io/presentations/2020-isugg-fpca-pfi/slides_with_SAND.pdf) 
